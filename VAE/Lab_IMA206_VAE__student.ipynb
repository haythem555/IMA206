{"cells":[{"cell_type":"markdown","metadata":{"id":"TdpPHz2Lp6VD"},"source":["# IMA 206 - Coding autoencoders in Pytorch\n","\n","The lab was originally created by Alasdair Newson (https://sites.google.com/site/alasdairnewson/)\n","\n","The current version is made by Loic Le Folgoc. If you have questions, please contact me at loic dot lefolgoc at telecom-paris dot fr.\n","\n","## Objective:\n","\n","The goal of this TP is to explore autoencoders and variational autoencoders applied to a simple dataset. In this first part, we will look at an autoencoder applied to MNIST. We recall that an autoencoder is a neural network with the following general architecture:\n","\n","\n","![AUTOENCODER](https://drive.google.com/uc?id=11dfNujSHa2-_eThp2aTpL1M_hLaEQX-G)\n","\n","\n","The tensor $z$ in the middle of the network is called a __latent code__, and it belongs to the latent space. It is this latent space which is interesting in autoencoders (for image synthesis, editing, etc).\n","\n","## Your task:\n","You need to add the missing parts in the code (parts between # --- START CODE HERE and # --- END CODE HERE or # FILL IN CODE or ...)"]},{"cell_type":"markdown","source":["First of all, let's load some packages:"],"metadata":{"id":"gp13aVUQq1WX"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"JqNeIJ8Op8Ao","executionInfo":{"status":"ok","timestamp":1716131309866,"user_tz":-60,"elapsed":4873,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable\n","from torchvision.utils import save_image\n","\n","import pdb\n","import numpy as np\n","import random\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["from tqdm import tqdm"],"metadata":{"id":"U6NKzRPlDKZp","executionInfo":{"status":"ok","timestamp":1716131309866,"user_tz":-60,"elapsed":7,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Fixing seed for reproduciablity\n","def set_seed(seed):\n","    torch.manual_seed(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n","        torch.backends.cudnn.benchmark = False\n","        torch.backends.cudnn.deterministic = True\n","\n","# Set the seed for reproducibility\n","set_seed(0)"],"metadata":{"id":"iEoF0KKA3_ww","executionInfo":{"status":"ok","timestamp":1716131309866,"user_tz":-60,"elapsed":6,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hyj5dj_eui9D"},"source":["First, we load the mnist dataset. I find that training on the full training dataset `mnist_trainset` is fast enough even on CPU (5-10 minutes), but should you need it, we create a reduced trainset below.\n","\n","Feel free to train on `mnist_trainset_reduced` instead if you prefer (results might be of poorer quality). To do so, replace the argument `mnist_trainset` in the `torch.utils.data.DataLoader(...)` call creating `mnist_train_loader` in the cell below by `mnist_trainset_reduced` (and same for `mnist_testset` and `mnist_testset_reduced`)."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"4YPLKlPrufSk","executionInfo":{"status":"ok","timestamp":1716131310297,"user_tz":-60,"elapsed":436,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"outputs":[],"source":["batch_size = 64\n","\n","# MNIST Dataset\n","mnist_trainset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n","mnist_testset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n","\n","#create data loader with smaller dataset size\n","max_mnist_size = 5000\n","mnist_trainset_reduced = torch.utils.data.random_split(mnist_trainset, [max_mnist_size, len(mnist_trainset)-max_mnist_size])[0]\n","mnist_train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=batch_size, shuffle=True, drop_last=False)\n","#mnist_train_loader = torch.utils.data.DataLoader(mnist_trainset_reduced, batch_size=batch_size, shuffle=True, drop_last=False)\n","\n","# download test dataset\n","max_mnist_size = 1000\n","mnist_testset_reduced = torch.utils.data.random_split(mnist_testset, [max_mnist_size, len(mnist_testset)-max_mnist_size])[0]\n","mnist_test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=batch_size, shuffle=False, drop_last=False)\n","#mnist_test_loader = torch.utils.data.DataLoader(mnist_testset_reduced, batch_size=batch_size, shuffle=False, drop_last=False)\n"]},{"cell_type":"code","source":["mnist_train_loader.dataset[0][0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JNu97xXfyKXO","executionInfo":{"status":"ok","timestamp":1716131310298,"user_tz":-60,"elapsed":9,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"outputId":"64d6e878-b336-49e8-caff-993135d06707"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 28, 28])"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"t-bkK4ktwfvC"},"source":["# 1. Vanilla Autoencoder\n","\n","Now, we define our autoencoder model. The autoencoder class `AEModel` is made of an `Encoder` and a `Decoder`, which we create first. We will reuse the `Encoder` and `Decoder` classes when building our variational autoencoder model, and wrap them in a `VAEModel` instead."]},{"cell_type":"markdown","metadata":{"id":"6jLa2-jQwxSI"},"source":["We will use the following convolutional architectures :\n","\n","__Encoder__ :\n","- Conv layer, 32 filters, 4x4 kernel, stride=2, padding=1; + ReLU\n","- Conv layer, 32 filters, 4x4 kernel, stride=2, padding=0; + ReLU\n","- Conv layer, 32 filters, 4x4 kernel, stride=2, padding=0; + ReLU\n","- Flatten\n","- Dense layer with 64 output neurons; + ReLU\n","- Dense layer with `self.latent_dim*self.multiplier` output neurons.\n","\n","For the autoencoder, `self.multiplier=1` as the encoder outputs a `self.latent_dim`-dimensional latent code. For the variational autoencoder, we will set `self.multiplier=2` as the encoder will output `self.latent_dim`-dimensional mean and log-variance parameters of the Gaussian distribution $q_\\phi(z|x)$.\n","\n","__Decoder__ (the decoder of the AE and VAE are the same, they always outputs an image/probability map, given a code $z$ as input):\n","- Dense layer with 64 output neurons; + ReLU\n","- Dense layer with ??? output neurons; + ReLU\n","- Reshape, to a `(C, H, W)` tensor with `C=32`, `H=???`, `W=???`.\n","- Conv transpose layer, 32 filters, 4x4 kernel, stride=2, padding=0; +ReLU\n","- Conv transpose layer, 32 filters, 4x4 kernel, stride=2, padding=0; +ReLU\n","- Conv transpose layer, 1 filter, 4x4 kernel, stride=2, padding=1; +Sigmoid\n","\n","The number of output neurons of the second dense layer is exactly the number of input neurons in the first dense layer of the encoder (i.e., the number of values in the feature maps of the conv layer immediately before it).\n","\n","For the reshape operations, use the ```A.view(dim_1,dim_2,...)``` function, where ```A``` is your tensor."]},{"cell_type":"markdown","source":["__Hint for computing the number of neurons that are not given to you__: This [great resource](https://madebyollin.github.io/convnet-calculator/) lets you compute the size of the output tensor following any convolution layer depending on the input tensor shape and conv parameters."],"metadata":{"id":"eNH7ScylwKa-"}},{"cell_type":"code","source":["class Encoder(torch.nn.Module):\n","    def __init__(self, latent_dim=10, multiplier=1):\n","        super(Encoder, self).__init__()\n","\n","        # Layer parameters\n","        self.latent_dim = latent_dim\n","        self.multiplier = multiplier\n","\n","        # Shape at the end of conv3\n","        self.reshape = (32,2,2)\n","\n","        # Convolutional layers\n","        self.conv1 = nn.Conv2d(in_channels=1,out_channels=32,kernel_size=4,stride = 2,padding=1)\n","        self.conv2 = nn.Conv2d(in_channels=32,out_channels=32,kernel_size=4,stride = 2,padding=0)\n","        self.conv3 = nn.Conv2d(in_channels=32,out_channels=32,kernel_size=4,stride = 2,padding=0)\n","\n","        # Fully connected layers\n","        self.fc1 = nn.Linear(128,64)\n","        self.fc2 = nn.Linear(64,self.latent_dim*self.multiplier)\n","\n","        self.activation  = nn.ReLU()\n","        self.flatten = nn.Flatten()\n","    def forward(self, x):\n","        '''\n","        Pass the input image mini-batch through conv, linear layers and\n","        non-linearities to output a (B,D,multiplier) tensor where B is the mini-batch\n","        size and D the latent dimension.\n","        '''\n","        batch_size = x.size(0)\n","\n","        # Convolutional layers with ReLu activations\n","        out = self.conv1(x)\n","        out = self.activation(out)\n","        out = self.conv2(out)\n","        out = self.activation(out)\n","        out = self.conv3(out)\n","        out = self.activation(out)\n","\n","        # Flatten\n","        out = self.flatten(out)\n","\n","        # Fully connected layer with ReLu activation\n","        out = self.fc1(out)\n","        out = self.activation(out)\n","\n","\n","        # Fully connected layer for code z, or mean and log-variance\n","        out = self.fc2(out)\n","        #out = self.activation(out)\n","\n","        # The shape of the output tensor should be (B,D) if multiplier=1,\n","        # where B is the batch size, and D the latent dimension.\n","        # Otherwise it should be (B,D,multiplier)\n","        if (self.multiplier == 2):\n","          out = out.view(batch_size,self.latent_dim,self.multiplier)\n","        return out"],"metadata":{"id":"0-IyPUQkIR-U","executionInfo":{"status":"ok","timestamp":1716131310298,"user_tz":-60,"elapsed":8,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","\n","    def __init__(self, latent_dim=10):\n","        super(Decoder, self).__init__()\n","\n","        self.latent_dim = latent_dim\n","        # Shape required to start transpose convs (copy paste from the encoder)\n","        self.reshape = (32,2,2)\n","\n","        # Fully connected layers\n","        self.fc1 = nn.Linear(self.latent_dim,64)\n","        self.fc2 = nn.Linear(64,128)\n","\n","        # Convolutional layers\n","        self.convT1 = nn.ConvTranspose2d(in_channels=32,out_channels=32,kernel_size = 4,stride = 2,padding=0)\n","        self.convT2 = nn.ConvTranspose2d(in_channels=32,out_channels=32,kernel_size = 4,stride = 2,padding=0)\n","        self.convT3 = nn.ConvTranspose2d(in_channels=32,out_channels=1,kernel_size = 4,stride = 2,padding=1)\n","\n","        self.activation = nn.ReLU()\n","        self.final_activation = nn.Sigmoid()\n","    def forward(self, z):\n","        batch_size = z.size(0)\n","\n","        # Fully connected layers with ReLu activations\n","        out = self.fc1(z)\n","        out = self.activation(out)\n","\n","        out = self.fc2(out)\n","        out = self.activation(out)\n","\n","        # Reshape\n","        out = out.view(batch_size,*self.reshape)\n","\n","        # Convolutional layers with ReLu activations\n","        out = self.convT1(out)\n","        out = self.activation(out)\n","        out = self.convT2(out)\n","        out = self.activation(out)\n","\n","        # Final conv layer with sigmoid activation\n","        out = self.convT3(out)\n","        out = self.final_activation(out)\n","\n","\n","        return out"],"metadata":{"id":"U64AqYvpYpr6","executionInfo":{"status":"ok","timestamp":1716131310298,"user_tz":-60,"elapsed":7,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["The autoencoder model itself is basically a wrapper around an `Encoder` and a `Decoder`. In the forward pass, the input images contained in the tensor `x` are passed through the `Encoder` to obtain the latent codes `z` then these codes are fed to the `Decoder` to produce the reconstructions `y`."],"metadata":{"id":"JmGyzVp4HVRJ"}},{"cell_type":"code","source":["class AEModel(nn.Module):\n","    def __init__(self, latent_dim):\n","        \"\"\"\n","        Class which defines model and forward pass.\n","\n","        Parameters\n","        ----------\n","        latent_dim : int\n","            Dimensionality of latent code.\n","        \"\"\"\n","        super(AEModel, self).__init__()\n","\n","        self.latent_dim = latent_dim\n","        self.encoder = Encoder(latent_dim = latent_dim,multiplier=1)\n","        self.decoder = Decoder(latent_dim = latent_dim)\n","\n","    def forward(self, x, mode='sample'):\n","        \"\"\"\n","        Forward pass of model, used for training or reconstruction.\n","\n","        Parameters\n","        ----------\n","        x : torch.Tensor\n","            Batch of data. Shape (batch_size, n_chan, height, width)\n","\n","        Outputs a dictionary containing:\n","          codes - the latent codes corresponding to the input images\n","          reconstructions - the images reconstructed by the autoencoder\n","        \"\"\"\n","\n","        # z is the output of the encoder\n","        z = self.encoder(x)\n","\n","        # Decode the samples to image space\n","        y = self.decoder(z)\n","\n","        # Return everything:\n","        return {\n","            'reconstructions': y,\n","            'codes': z\n","            }"],"metadata":{"id":"T9MYicH9Zf3o","executionInfo":{"status":"ok","timestamp":1716131310298,"user_tz":-60,"elapsed":7,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Next, we carefully create the reconstruction loss. It will be reused for the VAE loss later on."],"metadata":{"id":"VeWePstgIVBw"}},{"cell_type":"markdown","source":["The reconstruction loss translates a pixel-wise Bernoulli probabilistic model into a loss (`F.binary_cross_entropy`). It takes input images `data` and reconstructed probability maps `reconstructions` and computes the binary cross-entropy, from the two images."],"metadata":{"id":"3FIb4BDrInYy"}},{"cell_type":"code","source":["def reconstruction_loss(reconstructions, data):\n","    \"\"\"\n","    Calculates the reconstruction loss for a batch of data. I.e. negative\n","    log likelihood.\n","\n","    Parameters\n","    ----------\n","    data : torch.Tensor\n","        Input data (e.g. batch of images). Shape : (batch_size, 1,\n","        height, width).\n","\n","    reconstructions : torch.Tensor\n","        Reconstructed data. Shape : (batch_size, 1, height, width).\n","\n","    Returns\n","    -------\n","    loss : torch.Tensor\n","        Binary cross entropy, AVERAGED over images in the batch but SUMMED over\n","        pixel and channel.\n","    \"\"\"\n","    batch_size, n_chan, height, width = reconstructions.size()\n","\n","    # The pixel-wise loss is the binary cross-entropy, computed from\n","    # reconstructions and data. It is summed over pixels and averaged across\n","    # samples in the batch.\n","    loss_function = nn.BCELoss(reduction='none')\n","    loss_pixel_wise = loss_function(reconstructions,data)\n","    loss_batch_wise = loss_pixel_wise.sum(dim=[1, 2, 3]) # sum over all channels, and pixels\n","    loss = loss_batch_wise.mean()\n","    return loss"],"metadata":{"id":"CWrQbhOnbv7y","executionInfo":{"status":"ok","timestamp":1716131310298,"user_tz":-60,"elapsed":7,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["### Training the vanilla autoencoder"],"metadata":{"id":"iqibefFRJHy_"}},{"cell_type":"markdown","source":["The training proceeds as usual. We instantiate a model, move it to the correct device, create an optimizer and write the training loop."],"metadata":{"id":"RQbHTXMLJODw"}},{"cell_type":"code","source":["# Parameters\n","latent_dim = 10\n","\n","learning_rate = 1e-3\n","n_epoch = 5 # if running on GPU you can use more epochs (10 or more)"],"metadata":{"id":"NyZcTZP3a_kc","executionInfo":{"status":"ok","timestamp":1716131310298,"user_tz":-60,"elapsed":7,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ft_3txj0bZjO","outputId":"4540eae3-beea-4c5f-df78-90f94a0fecf6","executionInfo":{"status":"ok","timestamp":1716131310298,"user_tz":-60,"elapsed":6,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{"id":"oV40vRMQRoG1","executionInfo":{"status":"ok","timestamp":1716131310630,"user_tz":-60,"elapsed":336,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"outputs":[],"source":["# Model\n","ae_model = AEModel(latent_dim=latent_dim)\n","ae_model = ae_model.to(device)"]},{"cell_type":"code","source":["# Use the AdamW optimizer, set the correct learning rate and weight_decay to 1e-4\n","optimizer = optim.Adam(ae_model.parameters(),lr =learning_rate,weight_decay=1e-4)"],"metadata":{"id":"sbnKjRTDaynz","executionInfo":{"status":"ok","timestamp":1716131310630,"user_tz":-60,"elapsed":7,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["ae_model.train()\n","\n","for epoch in range(0,n_epoch):\n","  train_loss=0.0\n","\n","  with tqdm(mnist_train_loader, unit=\"batch\") as tepoch:\n","    for data, labels in tepoch:\n","      tepoch.set_description(f\"Epoch {epoch}\")\n","\n","      # Put data on correct device, GPU or CPU\n","      data = data.to(device)\n","\n","      # Pass the input data through the model\n","      predict = ae_model(data)\n","      reconstructions = predict['reconstructions']\n","\n","      # Compute the AE loss\n","      loss = reconstruction_loss(reconstructions,data)\n","\n","      # set the gradients back to 0\n","      optimizer.zero_grad()\n","\n","      # Backpropagate\n","      loss.backward()\n","      # parameter update\n","      optimizer.step()\n","\n","      # Aggregate the training loss for display at the end of the epoch\n","      train_loss += loss.item()\n","\n","      # tqdm bar displays the loss\n","      tepoch.set_postfix(loss=loss.item())\n","\n","  print('Epoch {}: Train Loss: {:.4f}'.format(epoch, train_loss/len(mnist_train_loader)))"],"metadata":{"id":"S2-phJydcICh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716131726183,"user_tz":-60,"elapsed":415560,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"outputId":"62ab5e2e-3ab8-4548-b377-5ae1dee67572"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 0: 100%|██████████| 938/938 [01:08<00:00, 13.67batch/s, loss=118]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0: Train Loss: 177.7462\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1: 100%|██████████| 938/938 [01:15<00:00, 12.37batch/s, loss=105]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Train Loss: 109.5765\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2: 100%|██████████| 938/938 [01:57<00:00,  7.96batch/s, loss=87.4]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Train Loss: 94.8464\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3: 100%|██████████| 938/938 [01:19<00:00, 11.86batch/s, loss=101]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3: Train Loss: 90.2010\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4: 100%|██████████| 938/938 [01:14<00:00, 12.62batch/s, loss=85.5]"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: Train Loss: 87.8265\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["### Testing the vanilla autoencoder"],"metadata":{"id":"iHscBN4KJ2Sq"}},{"cell_type":"markdown","metadata":{"id":"w3EbmswSzJdK"},"source":["We define functions for qualitative testing of the autoencoder model. We will reuse them throughout the lab."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"T8jXjdRyzMy2","executionInfo":{"status":"ok","timestamp":1716131726184,"user_tz":-60,"elapsed":46,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"outputs":[],"source":["def display_images(imgs):\n","  '''\n","  Display a batch of images (typically synthetic/generated images)\n","  '''\n","  r = 1\n","  c = imgs.shape[0]\n","  fig, axs = plt.subplots(r, c)\n","  for j in range(c):\n","    # black and white images\n","    axs[j].imshow(imgs[j, 0,:,:].detach().cpu().numpy(), cmap='gray')\n","    axs[j].axis('off')\n","  plt.show()\n","\n","def display_ae_images(ae_model, test_imgs):\n","  '''\n","  Display a batch of input images along with their reconstructions by a given model\n","    First row: input images\n","    Second row: reconstructed images\n","  '''\n","  n_images = 5\n","  idx = np.random.randint(0, test_imgs.shape[0], n_images)\n","  test_imgs = test_imgs[idx,:,:,:]\n","\n","  # get output images\n","  output_imgs = ae_model(test_imgs.to(ae_model.encoder.conv1.weight.device))['reconstructions']\n","  output_imgs = output_imgs.detach().cpu().numpy()\n","\n","  r = 2\n","  c = n_images\n","  fig, axs = plt.subplots(r, c)\n","  for j in range(c):\n","    axs[0,j].imshow(test_imgs[j, 0,:,:], cmap='gray')\n","    axs[0,j].axis('off')\n","    axs[1,j].imshow(output_imgs[j, 0,:,:], cmap='gray')\n","    axs[1,j].axis('off')\n","  plt.show()"]},{"cell_type":"markdown","source":["Let's see how well the autoencoder reconstructs images from the training set:"],"metadata":{"id":"TQiEplCiKnlT"}},{"cell_type":"code","execution_count":16,"metadata":{"id":"9pbXch29d68D","colab":{"base_uri":"https://localhost:8080/","height":324},"executionInfo":{"status":"ok","timestamp":1716131726633,"user_tz":-60,"elapsed":462,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"outputId":"9a443cf3-6918-40c8-f42e-c628e5c1c429"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 10 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# reconstructing training images\n","train_imgs = next(iter(mnist_train_loader))[0]\n","display_ae_images(ae_model, train_imgs)"]},{"cell_type":"markdown","source":["What about images from the test set?"],"metadata":{"id":"h3XxWKjGKtoA"}},{"cell_type":"code","source":["# reconstructing test images\n","test_imgs = next(iter(mnist_test_loader))[0]\n","display_ae_images(ae_model, test_imgs)"],"metadata":{"id":"ExMLThLofLn2","colab":{"base_uri":"https://localhost:8080/","height":324},"executionInfo":{"status":"ok","timestamp":1716131726894,"user_tz":-60,"elapsed":272,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"outputId":"37b6930e-fe58-47c6-b513-b48c35cbdebd"},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 10 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["There is not too much overfitting at work here apparently. We can quantify this by computing the reconstruction loss over the test dataset (below) and comparing it to the reconstruction loss over the training dataset at the end of training (check the training cell above)."],"metadata":{"id":"icWAOpmWKwid"}},{"cell_type":"code","source":["ae_model.eval()\n","test_loss=0.0\n","\n","# We will store all the latent codes corresponding to the test images for reuse\n","# later on.\n","zs_test = np.zeros((len(mnist_test_loader.dataset),ae_model.latent_dim))\n","\n","n = 0\n","with tqdm(mnist_test_loader, unit=\"batch\") as tepoch:\n","  for data, labels in tepoch:\n","    # Put the data on the correct device:\n","    data = data.to(device)\n","\n","    # Pass the data through the model\n","    predict = ae_model(data)\n","    reconstructions = predict['reconstructions']\n","    z = predict['codes']\n","\n","    # Compute the AE loss\n","    loss =  reconstruction_loss(reconstructions,data)\n","\n","    # Store quantities of interest\n","    minibatch_size = z.shape[0]\n","    zs_test[n:(n+minibatch_size),:] = z.detach().cpu().numpy()\n","\n","    # Compute the loss\n","    test_loss += loss.item()\n","\n","    # tqdm bar displays the loss\n","    tepoch.set_postfix(loss=loss.item())\n","\n","    # increment n to fill next parts of the arrays\n","    n += minibatch_size\n","\n","print('Test Loss: {:.4f}'.format(test_loss/len(mnist_test_loader)))"],"metadata":{"id":"HbyzjCqUjHQU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716131735036,"user_tz":-60,"elapsed":8161,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"outputId":"f1699e39-28e6-4adc-db03-fc441e3398cf"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 157/157 [00:08<00:00, 19.15batch/s, loss=77.1]"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 85.7155\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"assPaJqB5sa-"},"source":["The test and training average reconstruction losses are indeed similar."]},{"cell_type":"markdown","source":["Are you happy with the quality of the __reconstructions__? Next, we will see if this autoencoder model is good at __generating__ images."],"metadata":{"id":"SoLTYl8ELZsj"}},{"cell_type":"markdown","source":["The reconstructions seems pretty realistic and overall have the same shape in them. Furthermore, they are not identical pixel-wise which is good since we are sure like this that our model is able to have a good internal representation and is not overfitting"],"metadata":{"id":"B1AE4KJwExKD"}},{"cell_type":"markdown","metadata":{"id":"zO9ATQEiyr3b"},"source":["# 2. Image generation with the vanilla autoencoder\n","\n","Unfortunately, the vanilla autoencoder is not in itself a generative model because it does not define a joint probability distribution of the data and latent codes. We need to come up with roundabout ways to synthetize data based on this model."]},{"cell_type":"markdown","source":["In this section, we consider two naïve approaches to creating generative models from the AE. The general idea is the following:\n","\n","- train an autoencoder\n","- estimate different statistics (mean, variance) of the data in the latent space\n","- using these statistics, define a model based on a Gaussian distribution in the latent space\n","- generate latent codes with this distribution, then decode them back to image space to obtain synthetic images\n","\n","We will consider these two situations :\n","\n","- a multivariate Gaussian distribution with __diagonal covariance matrix__ (each latent variable is an independent random variable). This requires the mean and variance in each latent variable;\n","- a multivariate Gaussian distribution with __non-diagonal covariance matrix__. This requires the mean and covariance matrix of the latent codes.\n","\n","Obviously, since this is done _a posteriori_ after training the autoencoder, there is nothing which guarantees that the latent codes do indeed follow a Gaussian distribution. Our goal will be to verify that Variational Autoencoders indeed produce better results than such naïve approaches."],"metadata":{"id":"ECzGyqbyMaCt"}},{"cell_type":"markdown","metadata":{"id":"x2M1-BRmf56d"},"source":["### 2.0. Defining and generating random Gaussian latent codes\n","\n","Let $z$ be a latent code and $D$ the dimension of the latent space (called ``latent_dim`` in the code). We suppose that the $z$'s follow a multivariate Gaussian distribution, written as:\n","\n","\\begin{equation}\n","z \\sim \\mathcal{N}\\left(\n","\\mu,\n","\\bf{C}\n","\\right),\n","\\end{equation}\n","where $\\mu$ and $\\bf{C}$ are the mean vector and covariance matrix of the Gaussian distribution. To define such a generative model, we must therefore determine $\\mu$ and $\\bf{C}$. Once this is done, we can generate a random Gaussian latent code in the following manner:\n","\n","\\begin{equation}\n","z = \\mu + {\\bf{L}} \\varepsilon,\n","\\end{equation}\n","where $\\varepsilon$ is a random vector drawn from a multivariate normal distribution ($\\mu=0$ and ${\\bf{C}} = \\text{Id}$), and $\\bf{L}$ is output by a Cholesky decomposition of the positive semi-definite covariance matrix. In other words:\n","\n","\\begin{equation}\n","{\\bf{C}} = {\\bf{L}}{\\bf{L}^T}.\n","\\end{equation}\n","\n","This gives a simple method of producing a multivariate Gaussian random variable."]},{"cell_type":"markdown","metadata":{"id":"NWpucm972i7j"},"source":["### 2.1. A Gaussian model with diagonal covariance\n","\n","The first naïve model is  defined in this first case as:\n","\n","- $\\bf{\\mu}=\\left[\\mu_0, \\mu_1, \\cdots, \\mu_{d-1}\\right]^T$\n","- $\n","  \\bf{C} = \\begin{pmatrix}\n","\\sigma_0^2 & 0 & \\cdots & 0 \\\\\n","0 & \\sigma_1^2 & \\cdots & 0 \\\\\n","\\vdots & \\ddots & \\ddots & \\vdots \\\\\n","0 & 0 & \\cdots & \\sigma_{d-1}^2\n","\\end{pmatrix}$\n","\n","In this situation, therefore, the matrix $\\bf{L}$ can be calculated quite simply, as:\n","- $\n","  \\bf{L} = \\begin{pmatrix}\n","\\sigma_0 & 0 & \\cdots & 0 \\\\\n","0 & \\sigma_1 & \\cdots & 0 \\\\\n","\\vdots & \\ddots & \\ddots & \\vdots \\\\\n","0 & 0 & \\cdots & \\sigma_{d-1}\n","\\end{pmatrix}$"]},{"cell_type":"markdown","source":["We are going to compute the mean and the component-wise standard deviations from a batch of data. For simplicity you are going to use the latent codes `zs_test` corresponding to the test data to estimate these quantities.<br>\n","\n","It is actually bad practice, and it would be better to estimate them from the training dataset. We do not do so here for convenience because we have already computed `zs_test` above (we have verified above that overfitting was not a problem, so the difference between the two estimates should be minor)."],"metadata":{"id":"gZCIY7RsN4NY"}},{"cell_type":"code","execution_count":19,"metadata":{"id":"sUXHCtvW2iQ0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716131735036,"user_tz":-60,"elapsed":52,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"outputId":"c8798e2a-0c5b-4dcd-ed99-63d85aeddc50"},"outputs":[{"output_type":"stream","name":"stdout","text":["Average of latent codes: [  7.22028331  12.95404638  16.52935552  22.84269631   5.09262123\n"," -17.04702717  -4.57268441 -12.19493944  -2.83901724   7.36044871]\n","Standard deviation of latent codes: [ 8.73327296 10.01254043  7.38124362  9.86110605  9.55406743  8.42358051\n","  9.33853395  8.95613394  8.00462011 12.36779669]\n"]}],"source":["# zs_test is of shape (N,D) where N is the test dataset size and D the latent dimension\n","# Compute the vector of mean values and the vector of component-wise std's.\n","z_average = zs_test.mean(axis=0)\n","z_sigma = zs_test.std(axis=0)\n","\n","print(\"Average of latent codes:\",z_average)\n","print(\"Standard deviation of latent codes:\",z_sigma)"]},{"cell_type":"markdown","metadata":{"id":"Lrpc62ML9K4l"},"source":["Now, in the next cell generate data with this simple generative model using the approach described above. Display these images with the `display_images` function.\n","\n","__Hint__. You do not actually have to define the matrix $\\bf{L}$ in this case, an element-wise multiplication of two (properly chosen) vectors will suffice. To generate multivariate normal random variables you can use the following Pytorch function:\n","\n","- `torch.randn`\n","\n","To convert a numpy array to pytorch tensor, use `torch.from_numpy(...).float()`\n"]},{"cell_type":"code","source":["def generate_images_diagonal_gaussian(ae_model, z_average, z_sigma, n_images = 5):\n","    # Sample noise from a standard Gaussian distribution\n","    epsilon = torch.randn(n_images,*z_average.shape,dtype=torch.float32)\n","\n","    if(type(z_average) !=type(torch.tensor([0]))):\n","      z_average = torch.tensor(z_average,dtype=torch.float32)\n","\n","    if(type(z_sigma) !=type(torch.tensor([0]))):\n","      z_sigma = torch.tensor(z_sigma,dtype=torch.float32)\n","\n","    # Using epsilon, generate samples from N(mu,C)\n","    z_generated = (z_average + z_sigma*epsilon).to(torch.float32)\n","    # Decode back to image space\n","    imgs_generated = ae_model.decoder(z_generated.to(device))\n","\n","    return imgs_generated"],"metadata":{"id":"cRXyUkVeppif","executionInfo":{"status":"ok","timestamp":1716131735036,"user_tz":-60,"elapsed":47,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","execution_count":21,"metadata":{"id":"1_Tekii-9QEo","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1716131735348,"user_tz":-60,"elapsed":358,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"outputId":"67d7300c-0bf6-407e-9153-b2d94ba4429a"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 5 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["imgs_generated = generate_images_diagonal_gaussian(ae_model, z_average, z_sigma, n_images=5)\n","display_images(imgs_generated)"]},{"cell_type":"markdown","metadata":{"id":"xiNaEgLIloeA"},"source":["What do you think of these samples? Next let's try a slightly more sophisticated model."]},{"cell_type":"markdown","source":["This samples seem to be expressing a general idea about what we want to generated. However the digits are not as good as the samples from the training and test set. Thus this leads us to conclude that the latent samples we used are not good enough for this trained model. And that we can still generate better results given a better latent code"],"metadata":{"id":"PzgtXYKmLvgH"}},{"cell_type":"markdown","metadata":{"id":"WjVPfkRKYMSh"},"source":["### 2.2. Non-diagonal Gaussian model\n","\n","The second model uses a non-diagonal covariance matrix $\\bf{C}$ in the multivariate Gaussian distribution. In the next cell, calculate the mean and covariance matrix from `zs_test`.\n","\n","__Hint__. You can use the `np.cov` function. Make sure to put the data in the right format for this. Print the shape of z_covariance to verify that you have a matrix of the correct shape (the covariance matrix and not the Gram matrix)."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"ArXgre39CD2H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716131735348,"user_tz":-60,"elapsed":24,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"outputId":"62385a59-9dc1-452e-a7a4-971153db467d"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Average of latent codes: [  7.22028331  12.95404638  16.52935552  22.84269631   5.09262123\n"," -17.04702717  -4.57268441 -12.19493944  -2.83901724   7.36044871]\n","\n","Shape of Covariance matrix of latent codes: (10, 10)\n","\n","Covariance matrix of latent codes: [[ 76.27768439 -16.52163543  10.34207311  -8.97914151  12.24050641\n","  -16.92302578   0.60677847  -5.58144029  21.05290296 -21.46384382]\n"," [-16.52163543 100.26099187  -6.86048481   9.9384881   -4.08275955\n","  -18.8488198   -4.12524408  21.98991358  -5.03031618  69.25820476]\n"," [ 10.34207311  -6.86048481  54.48820623  -3.54283246 -20.68742952\n","   -1.16490216  -0.55005034  -6.0046447   28.27379729  -2.81367144]\n"," [ -8.97914151   9.9384881   -3.54283246  97.25113764  -6.47880215\n","   28.58138448   2.44569343   4.93644139 -35.14242348 -17.51713987]\n"," [ 12.24050641  -4.08275955 -20.68742952  -6.47880215  91.28933336\n","  -13.26570531  18.18130689 -28.35707841 -20.14818186 -10.99585046]\n"," [-16.92302578 -18.8488198   -1.16490216  28.58138448 -13.26570531\n","   70.96380492   2.67644513   4.41249697 -20.13458469 -42.72817886]\n"," [  0.60677847  -4.12524408  -0.55005034   2.44569343  18.18130689\n","    2.67644513  87.21693799 -33.58968156   0.41301482  22.58049968]\n"," [ -5.58144029  21.98991358  -6.0046447    4.93644139 -28.35707841\n","    4.41249697 -33.58968156  80.22035714  -4.21674224  15.7616141 ]\n"," [ 21.05290296  -5.03031618  28.27379729 -35.14242348 -20.14818186\n","  -20.13458469   0.41301482  -4.21674224  64.08035113   0.25559489]\n"," [-21.46384382  69.25820476  -2.81367144 -17.51713987 -10.99585046\n","  -42.72817886  22.58049968  15.7616141    0.25559489 152.97769273]]\n"]}],"source":["z_average = zs_test.mean(axis=0)\n","z_covariance = np.cov(zs_test,rowvar=False)\n","\n","print(\"\\nAverage of latent codes:\", z_average)\n","print(\"\\nShape of Covariance matrix of latent codes:\", z_covariance.shape)\n","print(\"\\nCovariance matrix of latent codes:\", z_covariance)"]},{"cell_type":"markdown","metadata":{"id":"JhXU8cnTZ0E8"},"source":["Now, generate some samples with this distribution. In this case, you actually have to calculate the Cholesky decomposition and find $\\bf{L}$. For this, you can use `np.linalg.cholesky`. Then compute the latent codes according to $z = \\mu + {\\bf{L}} \\varepsilon$.\n","\n","__Hint__. You can use `torch.matmul`. Pay attention to the dimension of `epsilon` to implement it correctly."]},{"cell_type":"code","source":["# adding epsilon to the covariance matrix to ensure positive definitness\n","epsilon = 1e-10\n","z_covariance += epsilon * np.eye(z_covariance.shape[0])\n","# calculate Cholesky decomposition of covariance matrix : C = L L^T\n","L = np.linalg.cholesky(z_covariance)\n","L.shape"],"metadata":{"id":"rSbYZLZGrcdR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716131735348,"user_tz":-60,"elapsed":18,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"outputId":"f677790d-4bc8-4f24-ec81-b498e937c8eb"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10, 10)"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","execution_count":24,"metadata":{"id":"zXGlJTZ7Z4ed","executionInfo":{"status":"ok","timestamp":1716131735348,"user_tz":-60,"elapsed":14,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"outputs":[],"source":["def generate_images_non_diagonal_gaussian(ae_model, z_average, L, n_images = 5):\n","\n","  # Generate noise according to a standard Gaussian distribution\n","  epsilon = torch.randn(n_images,*z_average.shape)\n","\n","  if(type(z_average) !=type(torch.tensor([0]))):\n","      z_average = torch.tensor(z_average,dtype=torch.float32)\n","\n","  if(type(L) !=type(torch.tensor([0]))):\n","      L = torch.tensor(L,dtype=torch.float32)\n","  # Sample latent codes using epsilon\n","  z_generated = (z_average + (torch.matmul(L,epsilon.unsqueeze(dim = 2))).squeeze(dim=2)   ).to(torch.float32)\n","\n","  # Decode back to image space\n","  imgs_generated = ae_model.decoder(z_generated.to(device))\n","\n","  return imgs_generated"]},{"cell_type":"markdown","source":["Generate images using this model now:"],"metadata":{"id":"xs0o_VUvR3jl"}},{"cell_type":"code","source":["imgs_generated = generate_images_non_diagonal_gaussian(ae_model, z_average, L, n_images = 5)\n","display_images(imgs_generated)"],"metadata":{"id":"kGCJVjbXsKLE","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1716131735794,"user_tz":-60,"elapsed":459,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"outputId":"ff5e5974-e7ab-4e20-b2eb-ae6b3299ea43"},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 5 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"qLtsdri6zKEm"},"source":["You should see some improvement, but we can do better than this. Thus, we turn to the variational autoencoder."]},{"cell_type":"markdown","metadata":{"id":"8UqeNhuSdnDt"},"source":["# 3. Variational autoencoder\n","\n","Now, we are going to create a variational autoencoder to carry out __image generation__. Let's first recall the idea of a variational autoencoder.\n","\n","### Main idea\n","\n","The main idea is to create an autoencoder whose latent codes follow a certain distribution (a Gaussian distribution in practice). This is done with two tools :\n","\n","- A specific architecture, where the encoder produces the mean and variance of the latent codes\n","- A specially designed loss function\n","\n","Once the VAE is trained, it is possible to sample in the latent space by producing random normal variables and simply decoding.\n","\n","### Architecture\n","\n","The architecture of the VAE model is the same as before (using `Encoder` with `multiplier=2` and `Decoder`). However the wrapper `VAEModel` will be a bit more complex as we need to implement the reparametrization trick. We will also implement the code to generate samples (for test time).\n","\n","### Variational Autoencoder loss\n","\n","The VAE loss consists in a reconstruction loss and a KL divergence term.\n","\n","- The reconstruction loss is the same `reconstruction_loss` as before. In other words, the reconstructions are compared to the input images using binary cross-entropy. The reconstructions are generated by sampling a latent code from $q(z|x)$ and decoding it back to image space.\n","\n","- You will implement the KL divergence term manually below."]},{"cell_type":"code","execution_count":26,"metadata":{"id":"6siMHQLheM4T","executionInfo":{"status":"ok","timestamp":1716131735795,"user_tz":-60,"elapsed":31,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"outputs":[],"source":["class VAEModel(nn.Module):\n","    def __init__(self, latent_dim):\n","        super(VAEModel, self).__init__()\n","\n","        self.latent_dim = latent_dim\n","        self.encoder = Encoder(latent_dim=latent_dim,multiplier = 2)\n","        self.decoder = Decoder(latent_dim=latent_dim)\n","\n","    def reparameterize(self, mean, logvar, mode='sample'):\n","        \"\"\"\n","        Samples from a normal distribution using the reparameterization trick.\n","\n","        Parameters\n","        ----------\n","        mean : torch.Tensor\n","            Mean of the normal distribution. Shape (batch_size, latent_dim)\n","\n","        logvar : torch.Tensor\n","            Diagonal log variance of the normal distribution. Shape (batch_size,\n","            latent_dim)\n","\n","        mode : 'sample' or 'mean'\n","            Returns either a sample from qzx, or just the mean of qzx. The former\n","            is useful at training time. The latter is useful at inference time as\n","            the mean is usually used for reconstruction, rather than a sample.\n","        \"\"\"\n","        if mode=='sample':\n","            # Implements the reparametrization trick (slide 43):\n","            std = torch.exp(logvar/2)\n","            eps = torch.randn(*mean.shape)\n","            eps = eps.to(device)\n","            return mean + std*eps\n","        elif mode=='mean':\n","            return mean\n","        else:\n","            return ValueError(\"Unknown mode: {mode}\".format(mode))\n","\n","    def forward(self, x, mode='sample'):\n","        \"\"\"\n","        Forward pass of model, used for training or reconstruction.\n","\n","        Parameters\n","        ----------\n","        x : torch.Tensor\n","            Batch of data. Shape (batch_size, n_chan, height, width)\n","\n","        mode : 'sample' or 'mean'\n","            Reconstructs using either a sample from qzx or the mean of qzx\n","        \"\"\"\n","\n","        # stats_qzx is the output of the encoder\n","        stats_qzx = self.encoder(x)\n","\n","        # Use the reparametrization trick to sample from q(z|x)\n","        samples_qzx = self.reparameterize(*stats_qzx.unbind(-1), mode=mode)\n","\n","        # Decode the samples to image space\n","        reconstructions = self.decoder(samples_qzx)\n","\n","        # Return everything:\n","        return {\n","            'reconstructions': reconstructions,\n","            'stats_qzx': stats_qzx,\n","            'samples_qzx': samples_qzx}\n","\n","    def sample_qzx(self, x):\n","        \"\"\"\n","        Returns a sample z from the latent distribution q(z|x).\n","\n","        Parameters\n","        ----------\n","        x : torch.Tensor\n","            Batch of data. Shape (batch_size, n_chan, height, width)\n","        \"\"\"\n","        stats_qzx = self.encoder(x)\n","        samples_qzx = self.reparameterize(*stats_qzx.unbind(-1))\n","        return samples_qzx\n","\n","    def sample_pz(self, N):\n","        samples_pz = torch.randn(N, self.latent_dim, device=self.encoder.conv1.weight.device)\n","        return samples_pz\n","\n","    def generate_samples(self, samples_pz=None, N=None):\n","        if samples_pz is None:\n","            if N is None:\n","                return ValueError(\"samples_pz and N cannot be set to None at the same time. Specify one of the two.\")\n","\n","            # If samples z are not provided, we sample N samples from the prior\n","            # p(z)=N(0,Id), using sample_pz\n","            samples_pz = self.sample_pz(N)\n","\n","        # Decode the z's to obtain samples in image space (here, probability\n","        # maps which can later be sampled from or thresholded)\n","        generations = self.decoder(samples_pz)\n","        return {'generations': generations}"]},{"cell_type":"markdown","source":["The KL divergence term is computed as per the regularization term in slide 45 i.e., for each data sample in the mini-batch:\n","$$\\frac{1}{2}\\sum_{j=1}^D (\\mu_j^2 + \\sigma_j^2 - 1 - \\log{\\sigma_j^2})$$"],"metadata":{"id":"MUsrzszm-Hnf"}},{"cell_type":"code","execution_count":27,"metadata":{"id":"-pc40PPM7adL","executionInfo":{"status":"ok","timestamp":1716131735795,"user_tz":-60,"elapsed":29,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"outputs":[],"source":["def kl_normal_loss(mean, logvar):\n","    \"\"\"\n","    Calculates the KL divergence between a normal distribution\n","    with diagonal covariance and a unit normal distribution.\n","\n","    Parameters\n","    ----------\n","    mean : torch.Tensor\n","        Mean of the normal distribution. Shape (batch_size, latent_dim) where\n","        D is dimension of distribution.\n","\n","    logvar : torch.Tensor\n","        Diagonal log variance of the normal distribution. Shape (batch_size,\n","        latent_dim)\n","    \"\"\"\n","    # To be consistent with the reconstruction loss, wetake the mean over the\n","    # minibatch (i.e., compute for each sample in the minibatch according to\n","    # the equation above, then take the mean).\n","    var = torch.exp(logvar)\n","    latent_kl_batch_wise = 0.5*(mean**2+var-1-logvar).sum(axis=1)\n","    latent_kl =  latent_kl_batch_wise.mean(axis=0)\n","\n","    return latent_kl"]},{"cell_type":"markdown","source":["The `BetaVAELoss` puts it all together as per slide 55."],"metadata":{"id":"WhJbXJ0y_8OI"}},{"cell_type":"code","execution_count":28,"metadata":{"id":"x_hr2EwiCRSv","executionInfo":{"status":"ok","timestamp":1716131735795,"user_tz":-60,"elapsed":28,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"outputs":[],"source":["class BetaVAELoss(object):\n","    \"\"\"\n","    Compute the Beta-VAE loss\n","\n","    Parameters\n","    ----------\n","        beta: (scalar) the weight assigned to the regularization term\n","    \"\"\"\n","\n","    def __init__(self, beta):\n","        self.beta = beta\n","\n","    def __call__(self, reconstructions, data, stats_qzx):\n","        stats_qzx = stats_qzx.unbind(-1)\n","\n","        # Reconstruction loss\n","        rec_loss = reconstruction_loss(reconstructions,data)\n","\n","        # KL loss\n","        kl_loss = kl_normal_loss(*stats_qzx)\n","\n","        # Total loss of beta-VAE\n","        loss = rec_loss + self.beta*kl_loss\n","\n","        return loss"]},{"cell_type":"markdown","source":["### Training the Variational Autoencoder"],"metadata":{"id":"3RGTCPZXWaz6"}},{"cell_type":"markdown","metadata":{"id":"hk_9fDIphlsi"},"source":["This follows the traditional pipeline that you are by now familiar with."]},{"cell_type":"code","execution_count":29,"metadata":{"id":"bOdRTeDAMJCO","executionInfo":{"status":"ok","timestamp":1716131735798,"user_tz":-60,"elapsed":31,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"outputs":[],"source":["latent_dim = 10\n","beta= 1.0\n","\n","learning_rate = 1e-3\n","n_epoch = 5 # use the same number of epochs as before for fairness"]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"id":"xGN3jpxqOOwg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716131735799,"user_tz":-60,"elapsed":31,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"outputId":"96377089-5840-41c8-b9ae-998946b022f7"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"code","execution_count":31,"metadata":{"id":"D5XnxIE1L1bI","executionInfo":{"status":"ok","timestamp":1716131735799,"user_tz":-60,"elapsed":26,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"outputs":[],"source":["vae_model = VAEModel(latent_dim)\n","vae_model = vae_model.to(device)"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"fBVo5s-dQCBb","executionInfo":{"status":"ok","timestamp":1716131735800,"user_tz":-60,"elapsed":26,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"outputs":[],"source":["# To keep it simple, we can leave beta at 1.0 for the beta-VAE loss\n","# Feel free to experiment with it to see different trade-offs between reconstruction\n","# and generation performance.\n","\n","vae_loss = BetaVAELoss(beta=beta)"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"GiqsBcP7KIT3","executionInfo":{"status":"ok","timestamp":1716131735800,"user_tz":-60,"elapsed":26,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"outputs":[],"source":["# AdamW, with learning rate set to the parameter above and weight decay to 1e-4\n","optimizer = optim.Adam(vae_model.parameters(),lr= learning_rate,weight_decay=1e-4)"]},{"cell_type":"code","source":["vae_model.train()\n","\n","for epoch in range(0,n_epoch):\n","  train_loss=0.0\n","\n","  with tqdm(mnist_train_loader, unit=\"batch\") as tepoch:\n","    for data, labels in tepoch:\n","      tepoch.set_description(f\"Epoch {epoch}\")\n","\n","      # Put data on correct device, GPU or CPU\n","      data = data.to(device)\n","\n","      # Pass the input data through the model\n","      predict = vae_model(data)\n","      reconstructions = predict['reconstructions']\n","      stats_qzx = predict['stats_qzx']\n","      #ep = 1e-12\n","      #stats_qzx_new= torch.stack([stats_qzx[:,:,0],torch.log(stats_qzx[:,:,1]+ep)] ,dim=2)\n","      #print(f\"this batch has :\\n stats_qzx mean = {predict['stats_qzx'][:,:,0].mean(axis=0)} std = {predict['stats_qzx'][:,:,1].mean(axis=0)}\")\n","      # Compute the beta-VAE loss\n","      loss = vae_loss(reconstructions,data,stats_qzx)\n","      #print(loss)\n","      # Backpropagate\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","      # Aggregate the training loss for display at the end of the epoch\n","      train_loss += loss.item()\n","\n","      # tqdm bar displays the loss\n","      tepoch.set_postfix(loss=loss.item())\n","\n","  print('Epoch {}: Train Loss: {:.4f}'.format(epoch, train_loss/len(mnist_train_loader)))"],"metadata":{"id":"z4KHRufgxNmR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716132352147,"user_tz":-60,"elapsed":616372,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"outputId":"8377b0ad-187b-4c0d-eb1c-5bdde01f37fb"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 0: 100%|██████████| 938/938 [00:56<00:00, 16.58batch/s, loss=132]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0: Train Loss: 192.0851\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1: 100%|██████████| 938/938 [01:09<00:00, 13.46batch/s, loss=120]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Train Loss: 131.7242\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2: 100%|██████████| 938/938 [02:25<00:00,  6.45batch/s, loss=138]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Train Loss: 120.7728\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3: 100%|██████████| 938/938 [02:56<00:00,  5.30batch/s, loss=117]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3: Train Loss: 116.9169\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4: 100%|██████████| 938/938 [02:47<00:00,  5.60batch/s, loss=120]"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: Train Loss: 114.7118\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["### Testing the VAE model"],"metadata":{"id":"3DFW0vPRXrSB"}},{"cell_type":"markdown","source":["Let's check how well the VAE reconstructs training samples:"],"metadata":{"id":"Who7o-hAXLhl"}},{"cell_type":"code","execution_count":35,"metadata":{"id":"NiRg43Bgx_MH","colab":{"base_uri":"https://localhost:8080/","height":324},"executionInfo":{"status":"ok","timestamp":1716132352526,"user_tz":-60,"elapsed":419,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"outputId":"094ea742-87a2-4078-af20-b44563e7ac4a"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 10 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# reconstructing training images\n","train_imgs = next(iter(mnist_train_loader))[0]\n","display_ae_images(vae_model, train_imgs)"]},{"cell_type":"markdown","source":["Same for test samples:"],"metadata":{"id":"Bqa9wmZ3XSxD"}},{"cell_type":"code","source":["# reconstructing test images\n","test_imgs = next(iter(mnist_test_loader))[0][:5]\n","#display_ae_images(vae_model, test_imgs)\n","reconstructed_images = vae_model(test_imgs,mode=\"mean\")['reconstructions']\n","display_images(test_imgs)\n","display_images(reconstructed_images)"],"metadata":{"id":"IRJ9-V0tx_MJ","colab":{"base_uri":"https://localhost:8080/","height":227},"executionInfo":{"status":"ok","timestamp":1716132352763,"user_tz":-60,"elapsed":247,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"outputId":"677fc518-afbe-4d5e-f9aa-cb4ec325635f"},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 5 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAgQAAABpCAYAAABF9zs7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAMhklEQVR4nO3dfYhU1R/H8bO55kOCmamVhOmKltm6PqRZaWVi6/NqbUoSmqAVSYJPZRaVWYKBoWYr/RGphUqp4aaVEaZFWhi2arsWGbmFipllmkVp+/vjh5++M9xx7szeOzN39v366zPrnZmD06yn873newrq6urqHAAAaNAuyvYAAABA9jEhAAAATAgAAAATAgAA4JgQAAAAx4QAAAA4JgQAAMAxIQAAAM65Qr8XFhQUhDmOBiuIvlB8NuGo72fD5xIOvjO5i+9MbvL7ubBCAAAAmBAAAAAmBAAAwDEhAAAAjgkBAABwTAgAAIBjQgAAABwTAgAA4FJoTAR4mTVrlnKzZs2Ui4uLle+55x7P51ZUVCjv3LlTefXq1UEOEQDgAysEAACACQEAAHCuoM5nk2N6TIcjin3Z161bp5yoHJCqgwcPKg8ePFi5trY2kNdPB33ZnevSpYvygQMHlKdPn668bNmyjI4pit8ZPy655BLlF198UfnBBx9U/vLLL5XLy8uVDx06FPLo/OE7k5s4ywAAAPjGhAAAALDLAP6kWiawy8sffPCBcqdOnZRHjhypXFRUpDxhwgTlhQsXpj5YBKZnz57K//77r/JPP/2UjeHktSuvvFJ5ypQpyvbvvXfv3sojRoxQXr58ecijy3+9evVS3rBhg/I111wT+HsNGTJEuaamRvnHH38M/L1SwQoBAABgQgAAACgZIIE+ffrEPB4zZozndV9//bXyqFGjlI8fP658+vRp5Ysvvlh5165dyj169FBu3bp1GiNGGEpKSpT/+OMP5Y0bN2ZhNPmnTZs2yitXrsziSHDXXXcpN2nSJNT3suXSyZMnK48fPz7U902GFQIAAMCEAAAAZKFkYO9Qt3fSOufc4cOHlf/66y/lN998U/no0aPK3333XRhDhIu949m52IYhtkxgl9mOHDmS9HVnzpyp3K1bN89rNm/e7HucCF737t2Vp02bpswZE8F49NFHlcvKypT79u2b0usMHDhQ+aKL/vt/u6qqKuUdO3akMcKGo7Dwv38Chw0blrH3tQ2mZsyYoWybUzkXW6bLBFYIAAAAEwIAAMCEAAAAuCzcQ7Bo0SJlvx2g7OEep06dUra17DDYbmx23M45t3v37lDfO9sqKytjHnfu3FnZfgYnTpxI6XXttprGjRunOTqE6dprr1W2NU3brRLpe+mll5RtF8JUjR071jPbg47GjRunbOvW+L877rhDuX///srxv++D1qpVK2V7L1Xz5s1jruMeAgAAkHFMCAAAQOZLBnarYXFxccyf2UMerrvuOmV76MTtt9+ufNNNNynbQyGuvvrqpOM4e/as8s8//6wcv93uvNra2pjH+V4yiFef89Znz56t3KVLF89rPv/8c8+MzJszZ46y/dwb2n/zQdqyZYuy3SKYql9++UXZdgDt0KGDcseOHZW/+OIL5UaNGqX9vvnEbqtds2aN8sGDB5VfeOGFUMcwevToUF8/XawQAAAAJgQAACALJYOPPvrIM8d7//33PX9u7860B6/YO2hvvPHGpOOwnRC//fZbZVu2uOyyy5TtchKSs2e1z58/X9kebnTs2DHluXPnKp85cybk0SGe3fFjD7ay341M3/Ecdbfddpty165dle3OAj+7DFasWKG8detW5ZMnTyoPGjRIed68eZ6v8/DDDytXVFQkfd989eSTTyrbXTSlpaXKthwTFPvvif1voz47TYLGCgEAAGBCAAAAslAyqK9ff/1Vedu2bZ7XXKgU4eXuu+9WtiWJffv2KdOUJTV22dmWCSz7d7p9+/bQx4TE7BKmZXfg4MLiG62tXbtW+fLLL0/6fLujY/369crPPvuscqJymn3u1KlTldu0aaNsm+00bdo05vkvv/yy8j///JN0rFFiD9RzLvYQI3tAXti7aGwpx5YJPv74Y+Xffvst1DEkwwoBAABgQgAAACJYMghK27ZtlV955RVl2zTE3h2fas/+huidd95RHjJkiOc1q1atUrZ3+yK7brjhBs+fh93TPZ8UFsb+OvVTJrClMnvOx/Hjx1N6b1syWLhwofLixYuVbZ/8+M9106ZNyvm2o6q8vDzmsf17sL/7w2DLSBMmTFA+d+6c8oIFC5SzXa5hhQAAADAhAAAADbhk8MgjjyjbO3HtLoZvvvkmo2OKInv2w80336zcpEkTZbv8aZfHwmj+Af/sWSAPPPCA8p49e5Q//PDDjI6pIbB3s0+ePFk51TJBInb53y5T+2nYli9atmypbP87jxd2gya748OWkGwDvES75bKBFQIAAMCEAAAANLCSwS233KL8+OOPe15TVlamvH///rCHFHm2gUrr1q09r3njjTeU8+0O5igbPHiwsu2zbs8RsWd+IDWJjjnu169fqO9bUFDgOYYLHbv8zDPPKN9///2hjCuTbMmyffv2MX9mjzwOW1FRkefPc/XfFlYIAAAAEwIAANDASga2h3Xjxo2V7dkHO3fuzOiYomjUqFHKvXr18rzG9ud++umnwx4S0tCjRw/luro65bfffjsbw4m8hx56KOZxto61HTlypHLPnj2VL3Tssi0Z5INTp04pf/XVVzF/VlxcrGxLZUE1n7NN7+LPUTjv008/DeS9gsYKAQAAYEIAAAAaQMmgWbNmyqWlpcp///23sl3SznYv6VxldxA88cQTyrb0YtllOhoQ5Y4rrrhCecCAAcq2CdfGjRszOqZ8YZfqM8E2VOvWrZuy/X4mEn+sdb793vvzzz+V43c22ePuN2/erGzPffCje/fuyp06dVK25xfYUpyVrXJSMqwQAAAAJgQAAKABlAxmz56tbO+4tc1XPvvss4yOKYpmzpypnKgnuj3+mJ0FuWnSpEnK9m7o9957LwujQX3MmzdP2Z7NksgPP/ygPHHixJg/q62tDWxcuSb+d5Ft3DR8+HDlVBsW2bMnbGnAz7HXr7/+ekrvlSmsEAAAACYEAAAgT0sGdhnoqaeeUv7999+V58+fn9ExRd2MGTOSXjNt2jRldhbkpg4dOnj+3B77jdy1ZcsW5a5du6b03OrqauVcbYwThgMHDsQ8vvfee5VLSkqUO3funNLrJmrgtXLlSmV7/LRld0HkElYIAAAAEwIAAJBHJQPbOGfp0qXKjRo1UrbLbbt27crMwBoQ2xc81UYnJ0+e9HyubXzUsmVLz+deeumlMY/9lDfOnTun/NhjjymfOXMm6XOjbMSIEZ4/r6yszPBI8o+9e925xMcNDx061PPnr776qvJVV13leY19zVSb22S6cVIU2AZq8WcepOv7779Peo1tapRLRyGzQgAAAJgQAACAiJcMbDnANhrq2LGjsu1jbXccIHh79+5N+7lvvfWW8pEjR5TbtWunPG7cuLRf/0KOHj2q/Pzzz4fyHtl06623KtuzDBCsioqKmMeLFi3yvO7dd99VTrTs76cc4OeaFStWJL0GwbKlo/gy0nm5VCawWCEAAABMCAAAQMRLBkVFRcq9e/f2vMbecR5/DCb8szs0Ro8eHfjrl5eXp3T92bNnlS+0dLpp0ybl3bt3e17zySefpPTeUTNmzBhlW2bbs2eP8o4dOzI6pny0YcOGmMf2HBV7VHFQ7BHGNTU1ylOnTlW25Tdkhj3XINHxx7mKFQIAAMCEAAAAMCEAAAAugvcQ2MNZtm7d6nmNrd3ZLT5I39ixY5XnzJmjbDsJJnL99dcr+9k6+NprrynbM9yt9evXK8cfXgLnmjdvrjxs2DDPa+zhLLZzI9Jz6NChmMfjx49XLisrU54+fXog72e3yC5fvjyQ10T9NW3a1PPnuXqgkcUKAQAAYEIAAACcK6jzuS8iUcelTLPLZHPnzvW8pm/fvsqJtprliiC2peTKZ5Nv6vvZZPNzsaWc7du3Kx87dkz5vvvuU47SoU5R/86UlpYq2y2C9vAhu13WHnpkx11dXa1cW1sb+DjTEeXvTFBs59PCwv+q8s8995zykiVLMjomv58LKwQAAIAJAQAAiEjJwB7OYjvmtWjRwvN6SgYIAsufuYnvTO7iO+NcZWWl8uLFi5W3bduWjeE45ygZAACAFDAhAAAA0WhMNGDAAOVEZQJ7cNHp06dDHxMAAPHsbpGoYYUAAAAwIQAAABEpGSRSVVWlfOeddyqfOHEiG8MBACCyWCEAAABMCAAAQEQaE+UzmqzkLpqs5Ca+M7mL70xuojERAADwjQkBAADwXzIAAAD5ixUCAADAhAAAADAhAAAAjgkBAABwTAgAAIBjQgAAABwTAgAA4JgQAAAAx4QAAAA45/4HF5vDRbbSGfgAAAAASUVORK5CYII=\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 5 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["Let's compute the quantitative `reconstruction_loss` on the test data:"],"metadata":{"id":"OR-uGTX2XaEB"}},{"cell_type":"code","source":["vae_model.eval()\n","test_loss=0.0\n","\n","# We will store all the latent codes corresponding to the test images for reuse\n","# later on.\n","zs_test = np.zeros((len(mnist_test_loader.dataset),vae_model.latent_dim))\n","\n","n = 0\n","with tqdm(mnist_test_loader, unit=\"batch\") as tepoch:\n","  for data, labels in tepoch:\n","    # Put the data on the correct device:\n","    data = data.to(device)\n","\n","    # Pass the data through the model\n","    predict = vae_model(data,mode=\"mean\")\n","    reconstructions = predict['reconstructions']\n","    z = predict['samples_qzx']\n","\n","    # Compute the AE loss\n","    loss =  reconstruction_loss(reconstructions,data)\n","\n","    # Store quantities of interest\n","    minibatch_size = z.shape[0]\n","    zs_test[n:(n+minibatch_size),:] = z.detach().cpu().numpy()\n","\n","    # Compute the loss\n","    test_loss += loss.item()\n","\n","    # tqdm bar displays the loss\n","    tepoch.set_postfix(loss=loss.item())\n","\n","    # increment n to fill next parts of the arrays\n","    n += minibatch_size\n","\n","print('Test Loss: {:.4f}'.format(test_loss/len(mnist_test_loader)))"],"metadata":{"id":"P0EzdVDzx_MJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716132370619,"user_tz":-60,"elapsed":17869,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"outputId":"c6d9919e-833b-4e69-f9a6-a59b5ae71624"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 157/157 [00:17<00:00,  8.86batch/s, loss=90.9]"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 94.3778\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["### Image generation with the VAE model"],"metadata":{"id":"wFOJ0yCzXuxO"}},{"cell_type":"markdown","metadata":{"id":"uTfRje_AkKDr"},"source":["Now, generate some images with the VAE model. You can directly use the `generate_samples` routine from the `VAEModel` class above."]},{"cell_type":"code","source":["zs_test.mean(axis=0)"],"metadata":{"id":"ModoKorilJPW","executionInfo":{"status":"ok","timestamp":1716134392811,"user_tz":-60,"elapsed":222,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"outputId":"490409ba-8452-4c4e-9254-41026ca7d650","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.03380275,  0.04040245,  0.06462632,  0.04285639, -0.03090221,\n","        0.09175596,  0.02273956, -0.01000646,  0.04701268, -0.0005183 ])"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["zs_test.std(axis=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YZBdMtK5jkN0","executionInfo":{"status":"ok","timestamp":1716133991151,"user_tz":-60,"elapsed":217,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"outputId":"01bda871-9c32-417d-9f40-c4a626a107ca"},"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.01070572, 0.11157047, 0.96210415, 1.02572296, 1.01941459,\n","       0.95808446, 0.94230096, 0.97524621, 0.07436101, 1.00336355])"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["def generate_images_vae(vae_model, n_images=5):\n","    return vae_model.generate_samples(N=n_images)['generations']"],"metadata":{"id":"NGt_LSDEE2vz","executionInfo":{"status":"ok","timestamp":1716132370619,"user_tz":-60,"elapsed":60,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["imgs_generated = vae_model.generate_samples(N=5)['generations']\n","display_images(imgs_generated)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122},"id":"42zbXkh4X4Qu","executionInfo":{"status":"ok","timestamp":1716132370620,"user_tz":-60,"elapsed":59,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"outputId":"f63a1e25-77e3-4065-bc17-6f1076eacda2"},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 5 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["As we can see, using only the z generated from a standard normal random multivariate  distribution is yields acceptable results"],"metadata":{"id":"gowjSCSZc7lg"}},{"cell_type":"markdown","metadata":{"id":"lGnvKoynzaFN"},"source":["Do you think the results are better ? What advantage does the Variational Autoencoder have over the simple autoencoder model, even though the second (non-diagonal Gaussian model) AE approach has a more complex probabilistic latent model (a full covariance matrix)?"]},{"cell_type":"markdown","source":["Yes, I think that overall the results of the VAE are better. The reconstructions seems visually to be having approximatly the same performance in the two variants of auto-encoders. However the generation step seems to be more effiecent in VAE yielding more realistic images. Bearing in mind that we know the prior p(z) for the vae we are more efficient in generating the samples. Thus the VAE presentes a huge advantage. Further more, the beat-VAE is much more flexibale. Since we can tune the parameter beta so we either focus on the reconstruction or rather on the generation. And in our case including the prior distribution of the latent space in the model allows us to get a much better approximation of the true distibution that the encoder is learning to yield. This is much more better than imposing nothing at all and then supposing a non-diagonal Gaussian model for it."],"metadata":{"id":"cuKiF7Rnhl8j"}},{"cell_type":"markdown","metadata":{"id":"uXm-D9Ef9vYm"},"source":["Next, we compare the models quantitavely."]},{"cell_type":"markdown","metadata":{"id":"04MddkzuE324"},"source":["# 4. Evaluating and comparing the models for image generation"]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"metadata":{"id":"D8WsuVqmaWmd","executionInfo":{"status":"ok","timestamp":1716132370620,"user_tz":-60,"elapsed":56,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3b40130d-1d14-4621-ac91-069cabc532f2"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}]},{"cell_type":"markdown","source":["Nowadays, the standard metric to evaluate the generative performance of a model is the FID (Fréchet Inception Distance). I leave it for you as optional homework to implement it if you wish to do so.\n","\n","Here, we will follow another path, a simplified version of the Inception Score (IS) that has been somewhat superseded by the FID:\n","\n","- we train a simple convolutional neural network classifier on MNIST, to a good accuracy\n","- we generate images with each model\n","- we find the average of the highest probability of the images according to the classifier, for each model. If this value is high, it means that on average the classifier considers that the images looks like a real image of an actual digit"],"metadata":{"id":"B93T6PD2YhgF"}},{"cell_type":"markdown","source":["We will use the following simple convolutional architecture for the classifier:\n","\n","- conv2d, 3x3 kernel, 32 filters, stride=2, padding=1; + ReLU + BatchNorm2d\n","- conv2d, 3x3 kernel, 64 filters, stride=2, padding=1; + ReLU + BatchNorm2d\n","- conv2d, 3x3 kernel, 128 filters, stride=2, padding=1; + ReLU + BatchNorm2d\n","- Global Average Pooling\n","- Flatten\n","- Dense layer\n","\n","Now, define the model. To make things easier, we use the `torch.nn.Sequential` API to implement the model (there is no need for a Class in this simple case).\n","\n","__Hint__. For the global average pooling, use the `torch.nn.AvgPool2d` layer with the suitable kernel size and stride."],"metadata":{"id":"Fk1OdN6jZZf1"}},{"cell_type":"code","execution_count":41,"metadata":{"id":"P87a-DkXFOCv","executionInfo":{"status":"ok","timestamp":1716132370620,"user_tz":-60,"elapsed":52,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"outputs":[],"source":["nb_classes = 10\n","kernel_size = (3, 3)\n","\n","mnist_classification_model = torch.nn.Sequential(\n","        nn.Conv2d(1,32,3,2,1),nn.ReLU(),nn.BatchNorm2d(32),\n","        nn.Conv2d(32,64,3,2,1),nn.ReLU(),nn.BatchNorm2d(64),\n","        nn.Conv2d(64,128,3,2,1),nn.ReLU(),nn.BatchNorm2d(128),\n","        nn.AvgPool2d(4),\n","        nn.Flatten(),\n","        nn.Linear(128,10)\n","    )\n","\n","mnist_classification_model = mnist_classification_model.to(device)"]},{"cell_type":"code","source":["learning_rate = 1e-2\n","n_epoch = 5"],"metadata":{"id":"rJ6ZYkKVaY64","executionInfo":{"status":"ok","timestamp":1716132370914,"user_tz":-60,"elapsed":345,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["# Cross entropy with reduction='sum'\n","criterion = nn.CrossEntropyLoss(reduction='sum')"],"metadata":{"id":"Zexnteq71bJ5","executionInfo":{"status":"ok","timestamp":1716132370914,"user_tz":-60,"elapsed":37,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["# AdamW, weight decay set to 1e-4\n","optimizer = optim.Adam(mnist_classification_model.parameters(),lr=learning_rate,weight_decay =1e-4)"],"metadata":{"id":"pWkFh6zHajBg","executionInfo":{"status":"ok","timestamp":1716132370915,"user_tz":-60,"elapsed":37,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["input = torch.randn(2,1,28,28)\n","out = mnist_classification_model(input)\n","out.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UomDfvkUojN4","executionInfo":{"status":"ok","timestamp":1716132370915,"user_tz":-60,"elapsed":36,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"outputId":"de339ac5-9f5e-453c-e712-89fd2a44dcaf"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 10])"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["def vector_to_class(x):\n","  y = torch.argmax(F.softmax(x,dim=1),axis=1)\n","  return y"],"metadata":{"id":"eBZrmVV42Gej","executionInfo":{"status":"ok","timestamp":1716132370916,"user_tz":-60,"elapsed":33,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","execution_count":47,"metadata":{"id":"aOww0ydr2fT0","executionInfo":{"status":"ok","timestamp":1716132370916,"user_tz":-60,"elapsed":32,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"outputs":[],"source":["def cnn_accuracy(x_pred,x_label):\n","  acc = (x_pred == x_label).sum()/(x_pred.shape[0])\n","  return acc"]},{"cell_type":"code","source":["imgs,labels =next(iter(mnist_train_loader))\n","print(f\"labels.shape {labels.shape}, imgs.shape{imgs.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OXaqmYIyo_p9","executionInfo":{"status":"ok","timestamp":1716132370916,"user_tz":-60,"elapsed":32,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"outputId":"60a83657-5764-446b-f45d-49f3b3b763ba"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["labels.shape torch.Size([64]), imgs.shapetorch.Size([64, 1, 28, 28])\n"]}]},{"cell_type":"code","execution_count":49,"metadata":{"id":"0FA8YoX2FcHP","executionInfo":{"status":"ok","timestamp":1716132627507,"user_tz":-60,"elapsed":256619,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"873753a4-a04d-482c-e12c-0016cdc88482"},"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 0: 100%|██████████| 938/938 [00:50<00:00, 18.59batch/s, loss=0.751]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch:0 Train Loss:0.1668 Accuracy:0.9484\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1: 100%|██████████| 938/938 [00:50<00:00, 18.48batch/s, loss=0.747]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch:1 Train Loss:0.0618 Accuracy:0.9802\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2: 100%|██████████| 938/938 [00:51<00:00, 18.16batch/s, loss=0.432]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch:2 Train Loss:0.0486 Accuracy:0.9847\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3: 100%|██████████| 938/938 [00:52<00:00, 17.84batch/s, loss=1.1]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch:3 Train Loss:0.0397 Accuracy:0.9869\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4: 100%|██████████| 938/938 [00:51<00:00, 18.32batch/s, loss=0.753]"]},{"output_type":"stream","name":"stdout","text":["Epoch:4 Train Loss:0.0364 Accuracy:0.9883\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["mnist_classification_model.train()\n","\n","for epoch in range(0,n_epoch):\n","  train_loss=0.0\n","  all_labels = []\n","  all_predicted = []\n","\n","  with tqdm(mnist_train_loader, unit=\"batch\") as tepoch:\n","    for imgs, labels in tepoch:\n","      tepoch.set_description(f\"Epoch {epoch}\")\n","\n","      # Put data on correct device\n","      imgs = imgs.to(device)\n","      labels = labels.to(device)\n","\n","      # forward pass and loss computation\n","      predict= mnist_classification_model(imgs)\n","      loss= criterion(predict,labels)\n","\n","      # backpropagation\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","      # compute the train loss\n","      train_loss += loss.item()\n","\n","      # store labels and class predictions\n","      all_labels.extend(labels.tolist())\n","      all_predicted.extend(vector_to_class(predict).tolist())\n","\n","      # tqdm bar displays the loss\n","      tepoch.set_postfix(loss=loss.item())\n","\n","  print('Epoch:{} Train Loss:{:.4f} Accuracy:{:.4f}'.format(epoch,train_loss/len(mnist_train_loader.dataset),\n","                                                            cnn_accuracy(np.array(all_predicted),np.array(all_labels))))"]},{"cell_type":"code","source":["mnist_classification_model.eval()\n","\n","all_predicted = []\n","all_labels = []\n","\n","with tqdm(mnist_test_loader, unit=\"batch\") as tepoch:\n","  for imgs, labels in tepoch:\n","    all_labels.extend(labels.tolist())\n","\n","    imgs = imgs.to(device)\n","    predict=mnist_classification_model(imgs)\n","    all_predicted.extend(vector_to_class(predict).tolist())\n","\n","test_accuracy = cnn_accuracy(np.array(all_predicted),np.array(all_labels))\n","\n","print(\"\\nTest Accuracy:\", test_accuracy)"],"metadata":{"id":"5bRM-1_x4bu2","executionInfo":{"status":"ok","timestamp":1716132630473,"user_tz":-60,"elapsed":3028,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3c0af83d-a62d-4160-f18f-2a4925faf719"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 157/157 [00:02<00:00, 55.20batch/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Test Accuracy: 0.9821\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"zFgN5LblFwTa"},"source":["### Evaluate the average maximum prediction of the images generated by each generative model (higher is better)\n","\n","Now, we evaluate the models. For each ones, produce a certain number of images, and put those images through the classification network. Then find the maximum class probability of each image, and average it over all the images. We will use this as a metric to evaluate each model.\n","\n","__CAREFUL__: the output of the network does __not__ include the Softmax layer, so you will have to carry it out, with:\n","- ```torch.nn.Softmax()(...)```\n","\n","Define this metric now:"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"lCJ_0qqjOXHT","executionInfo":{"status":"ok","timestamp":1716132630473,"user_tz":-60,"elapsed":16,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}}},"outputs":[],"source":["def generative_model_score(imgs_in,classification_model):\n","    logits =classification_model(imgs_in)\n","    probabilties = torch.nn.Softmax(dim=1)(logits)\n","    highest_probabilty_per_image = probabilties.max(axis=1)[0]\n","    gen_score = highest_probabilty_per_image.mean()\n","    return gen_score"]},{"cell_type":"markdown","metadata":{"id":"yGq7YFg51UoP"},"source":["Now, generate some images with each of the three models, and evaluate these models:"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"4-L4u2jhILFx","executionInfo":{"status":"ok","timestamp":1716132634689,"user_tz":-60,"elapsed":4230,"user":{"displayName":"haythem daghmoura","userId":"15535753170728038854"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"eef555d7-b63a-4b5d-9b5c-b0ab1c5a582c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Diagonal gaussian generative model score :  0.8685743808746338\n","Non diagonal gaussian generative model score :  0.8900420069694519\n","Variational autoencoder model score:  0.9060139060020447\n"]}],"source":["imgs_diagonal_gaussian = generate_images_diagonal_gaussian(ae_model,z_average,z_sigma,n_images = 2000)\n","imgs_non_diagonal_gaussian = generate_images_non_diagonal_gaussian(ae_model,z_average,L,n_images = 2000)\n","imgs_vae = generate_images_vae(vae_model,n_images=2000)\n","\n","# average of maximum of first model\n","diagonal_gaussian_score = float(generative_model_score(imgs_diagonal_gaussian,mnist_classification_model))\n","non_diagonal_gaussian_score = float(generative_model_score(imgs_non_diagonal_gaussian,mnist_classification_model))\n","vae_gaussian_score = float(generative_model_score(imgs_vae,mnist_classification_model))\n","\n","print(\"Diagonal gaussian generative model score : \",diagonal_gaussian_score)\n","print(\"Non diagonal gaussian generative model score : \",non_diagonal_gaussian_score)\n","print(\"Variational autoencoder model score: \",vae_gaussian_score)"]},{"cell_type":"markdown","metadata":{"id":"sxvsG8FC1gNS"},"source":["Questions:\n","\n","- Which model is better quantitatively ? (unfortunately there is some variability, even with 2000 samples; you might want to rerun the cell several times to get the trend)\n","- Do the quantitative result support the qualitative results ?\n","- Can you see any drawbacks of this method of evaluation ?\n","- Can you propose any more sophisticated models than the multivariate Gaussian approach (apart from the variational autoencoder) ?"]},{"cell_type":"markdown","source":["Quantitavely the best model is the Variational autoencoder model yielding 0.906 fllowing it the Non diagonal gaussian generative model then the diagonal gaussian. This indicates that the quantitave results support the qualitative ones.\n","\n","This methods does suffer from two drawbacks. First the classifier Bias issue : Since the evaluation is highly dependent on the performance and biases of the pre-trained classifier. If the classifier is not robust, it might misrepresent the quality of generated images.\n","\n","Secondly, mode Collapse: This metric might not effectively capture the diversity of generated images. A model might generate similar images with high confidence scores, leading to misleadingly high scores.\n","\n","Other ideas might include Generative Adversarial Networks (GANs): GANs are powerful generative models that often produce high-quality images by training a generator and a discriminator in a competitive setting and where we don't need labeled data.\n","Or we might use Diffusion models which have proven to be very performant in generating new realistic images. They work by modeling the distribution of the data as a gradual denoising process, starting from pure noise and progressively refining it into a coherent image."],"metadata":{"id":"9TkE0SDTM4dw"}}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}